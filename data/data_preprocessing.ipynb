{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "--- \n",
    "This notebook sets out to clean up the data\n",
    "\n",
    "Mainly, we are looking to encode any string columns, and fill any NaN entries, and balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"raw_data/application_train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(r\"raw_data/application_test.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356255, 121)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307511, 121), (48744, 120))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "df_train.shape, df_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to created and implement an encoding dictionary\n",
    "\n",
    "\n",
    "def encode_data(df_train, df_test):\n",
    "    key = {}\n",
    "    key_columns = []\n",
    "\n",
    "    print(\"Creating encoding dictionary\")\n",
    "    # create a key for data in columns\n",
    "    for col in df_train.columns:\n",
    "        # check if the column is a string\n",
    "        if df_train[col].dtype == \"O\":\n",
    "            key_columns.append(col)\n",
    "            # loop over the unique strings in the dataframe\n",
    "            for col_name in df_train[col].unique():\n",
    "                # check if the string is not in the dictionary\n",
    "                if col_name not in key:\n",
    "                    # add to the dictionary with a unique value\n",
    "                    key[col_name] = len(key) + 1\n",
    "\n",
    "    print(\"Dictionary created...\")\n",
    "    print(\"Integrating keys into dataframe...\")\n",
    "    # replace the string values in the dataframe with the key created\n",
    "    for col in key_columns:\n",
    "        df_train = df_train.replace({str(col): key})  # train\n",
    "    for col in key_columns:\n",
    "        df_test = df_test.replace({str(col): key})  # test\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    return df_train, df_test, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating encoding dictionary\n",
      "Dictionary created...\n",
      "Integrating keys into dataframe...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test, key = encode_data(df_train, df_test)\n",
    "# key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns that are not int or float\n"
     ]
    }
   ],
   "source": [
    "# check that all the columns are numbers (empty is good)\n",
    "col_dtype = len(\n",
    "    list(df_train.select_dtypes(exclude=[\"int64\", \"float64\"]).columns)\n",
    ") + len(list(df_test.select_dtypes(exclude=[\"int64\", \"float64\"]).columns))\n",
    "\n",
    "print(f\"There are {col_dtype} columns that are not int or float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61 columns containing NaN values\n"
     ]
    }
   ],
   "source": [
    "# check and return for any columns with NaN\n",
    "print(\n",
    "    f\"There are {len(df_train.columns[df_train.isna().any()])} columns containing NaN values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fill NaN values with 0\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    df = df.fillna(value=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns containing NaN values\n"
     ]
    }
   ],
   "source": [
    "df_train = process_data(df_train)\n",
    "df_test = process_data(df_test)\n",
    "\n",
    "# check that there are not any NaN's (empty is good)\n",
    "nan_col = len(df_train.columns[df_train.isna().any()]) + len(\n",
    "    df_test.columns[df_test.isna().any()]\n",
    ")\n",
    "\n",
    "print(f\"There are {nan_col} columns containing NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe\n",
    "undersampled_data = pd.DataFrame()\n",
    "\n",
    "# split dataframe by defualt/non_default\n",
    "default = df_train[df_train[\"TARGET\"] == 1]\n",
    "non_default = df_train[df_train[\"TARGET\"] == 0]\n",
    "\n",
    "# reduce the larger sample to the smaller sample size\n",
    "non_default = non_default.sample(len(default), random_state=42)\n",
    "\n",
    "# add to new dataframe\n",
    "undersampled_data = pd.concat([default, non_default], axis=0)\n",
    "\n",
    "# shuffle dataframe\n",
    "undersampled_data = undersampled_data.sample(frac=1)\n",
    "\n",
    "# undersampled_data[undersampled_data[\"TARGET\"] == 1].shape, undersampled_data[undersampled_data[\"TARGET\"] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported\n"
     ]
    }
   ],
   "source": [
    "# export the cleaned data to a new file\n",
    "\n",
    "undersampled_data.to_csv(r\"processed_data/df_train.csv\")\n",
    "df_test.to_csv(r\"processed_data/df_test.csv\")\n",
    "\n",
    "print(\"Data exported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "--- \n",
    "This notebook sets out to clean up the data\n",
    "\n",
    "Mainly, we are looking to encode any string columns, and fill any NaN entries, and balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"raw_data/application_train.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 121)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 121)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "df_train.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to created and implement an encoding dictionary\n",
    "\n",
    "\n",
    "def encode_data(df_train):\n",
    "    key = {}\n",
    "    key_columns = []\n",
    "\n",
    "    print(\"Creating encoding dictionary\")\n",
    "    # create a key for data in columns\n",
    "    for col in df_train.columns:\n",
    "        # check if the column is a string\n",
    "        if df_train[col].dtype == \"O\":\n",
    "            key_columns.append(col)\n",
    "            # loop over the unique strings in the dataframe\n",
    "            for col_name in df_train[col].unique():\n",
    "                # check if the string is not in the dictionary\n",
    "                if col_name not in key:\n",
    "                    # add to the dictionary with a unique value\n",
    "                    key[col_name] = len(key) + 1\n",
    "\n",
    "    print(\"Dictionary created...\")\n",
    "    print(\"Integrating keys into dataframe...\")\n",
    "    # replace the string values in the dataframe with the key created\n",
    "    for col in key_columns:\n",
    "        df_train = df_train.replace({str(col): key}) \n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    return df_train, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating encoding dictionary\n",
      "Dictionary created...\n",
      "Integrating keys into dataframe...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "df_train, key = encode_data(df_train)\n",
    "# key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns that are not int or float\n"
     ]
    }
   ],
   "source": [
    "# check that all the columns are numbers (empty is good)\n",
    "col_dtype = len(\n",
    "    list(df_train.select_dtypes(exclude=[\"int64\", \"float64\"]).columns)\n",
    ")\n",
    "\n",
    "print(f\"There are {col_dtype} columns that are not int or float\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Fill any NaN entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 61 columns containing NaN values\n"
     ]
    }
   ],
   "source": [
    "# check and return for any columns with NaN\n",
    "print(\n",
    "    f\"There are {len(df_train.columns[df_train.isna().any()])} columns containing NaN values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fill NaN values with 0\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    df = df.fillna(value=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns containing NaN values\n"
     ]
    }
   ],
   "source": [
    "df_train = process_data(df_train)\n",
    "\n",
    "# check that there are not any NaN's (empty is good)\n",
    "nan_col = len(df_train.columns[df_train.isna().any()])\n",
    "\n",
    "print(f\"There are {nan_col} columns containing NaN values\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERSAMPLING\n",
    "x = df_train.drop([\"TARGET\"], axis=1)\n",
    "y = df_train.filter([\"TARGET\"], axis=1)\n",
    "\n",
    "smote = SMOTE()\n",
    "x_resampled, y_resampled = smote.fit_resample(x, y)\n",
    "\n",
    "oversampled_df = pd.DataFrame()\n",
    "oversampled_df = pd.concat([x_resampled, y_resampled], axis=1)\n",
    "# shuffle dataframe\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# oversampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDERSAMPLING\n",
    "\n",
    "# split dataframe by defualt/non_default\n",
    "default = df_train[df_train[\"TARGET\"] == 1]\n",
    "non_default = df_train[df_train[\"TARGET\"] == 0]\n",
    "\n",
    "# reduce the larger sample to the smaller sample size\n",
    "non_default = non_default.sample(len(default), random_state=42)\n",
    "\n",
    "# add to new dataframe\n",
    "undersampled_df = pd.DataFrame()\n",
    "undersampled_df = pd.concat([default, non_default], axis=0)\n",
    "# shuffle dataframe\n",
    "undersampled_df = undersampled_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# undersampled_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Split the dataset train/test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, test_size=0.2):\n",
    "    train_size = 1 - test_size\n",
    "    train = df[:int(len(df)*train_size)]\n",
    "    test = df[int(len(df)*train_size):]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_train, undersampled_test = split_dataset(undersampled_df, test_size=0.15)\n",
    "oversampled_train, oversampled_test = split_dataset(oversampled_df, test_size=0.15)\n",
    "\n",
    "# oversampled_train.shape, oversampled_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Export the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported\n"
     ]
    }
   ],
   "source": [
    "# export the cleaned data to a new file\n",
    "\n",
    "# undersampled\n",
    "undersampled_train.to_csv(r\"processed_data/undersampled_train.csv\")\n",
    "undersampled_test.to_csv(r\"processed_data/undersampled_test.csv\")\n",
    "\n",
    "# oversampled\n",
    "oversampled_train.to_csv(r\"processed_data/oversampled_train.csv\")\n",
    "oversampled_test.to_csv(r\"processed_data/oversampled_test.csv\")\n",
    "\n",
    "print(\"Data exported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

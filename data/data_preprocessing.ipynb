{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "--- \n",
    "This notebook sets out to clean up the data\n",
    "\n",
    "Mainly, we are looking to encode any string columns, and fill any NaN entries, and balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"raw_data/application_train.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 121)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [col for col in data.columns if data[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=categorical_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### NaN entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any columns with more than 80% missing\n",
    "\n",
    "data = data[data.columns[data.isnull().mean() < 0.80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For now, 61 features have null data\n",
      "AMT_ANNUITY have 12 null data\n",
      "------------------------------ 0 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 60 features have null data\n",
      "AMT_GOODS_PRICE have 278 null data\n",
      "------------------------------ 1 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 59 features have null data\n",
      "OWN_CAR_AGE have 202929 null data\n",
      "------------------------------ 2 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 58 features have null data\n",
      "CNT_FAM_MEMBERS have 2 null data\n",
      "------------------------------ 3 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 57 features have null data\n",
      "EXT_SOURCE_1 have 173378 null data\n",
      "------------------------------ 4 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 56 features have null data\n",
      "EXT_SOURCE_2 have 660 null data\n",
      "------------------------------ 5 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 55 features have null data\n",
      "EXT_SOURCE_3 have 60965 null data\n",
      "------------------------------ 6 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 54 features have null data\n",
      "APARTMENTS_AVG have 156061 null data\n",
      "------------------------------ 7 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 53 features have null data\n",
      "BASEMENTAREA_AVG have 179943 null data\n",
      "------------------------------ 8 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 52 features have null data\n",
      "YEARS_BEGINEXPLUATATION_AVG have 150007 null data\n",
      "------------------------------ 9 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 51 features have null data\n",
      "YEARS_BUILD_AVG have 204488 null data\n",
      "------------------------------ 10 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 50 features have null data\n",
      "COMMONAREA_AVG have 214865 null data\n",
      "------------------------------ 11 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 49 features have null data\n",
      "ELEVATORS_AVG have 163891 null data\n",
      "------------------------------ 12 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 48 features have null data\n",
      "ENTRANCES_AVG have 154828 null data\n",
      "------------------------------ 13 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 47 features have null data\n",
      "FLOORSMAX_AVG have 153020 null data\n",
      "------------------------------ 14 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 46 features have null data\n",
      "FLOORSMIN_AVG have 208642 null data\n",
      "------------------------------ 15 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 45 features have null data\n",
      "LANDAREA_AVG have 182590 null data\n",
      "------------------------------ 16 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 44 features have null data\n",
      "LIVINGAPARTMENTS_AVG have 210199 null data\n",
      "------------------------------ 17 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 43 features have null data\n",
      "LIVINGAREA_AVG have 154350 null data\n",
      "------------------------------ 18 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 42 features have null data\n",
      "NONLIVINGAPARTMENTS_AVG have 213514 null data\n",
      "------------------------------ 19 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 41 features have null data\n",
      "NONLIVINGAREA_AVG have 169682 null data\n",
      "------------------------------ 20 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 40 features have null data\n",
      "APARTMENTS_MODE have 156061 null data\n",
      "------------------------------ 21 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 39 features have null data\n",
      "BASEMENTAREA_MODE have 179943 null data\n",
      "------------------------------ 22 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 38 features have null data\n",
      "YEARS_BEGINEXPLUATATION_MODE have 150007 null data\n",
      "------------------------------ 23 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 37 features have null data\n",
      "YEARS_BUILD_MODE have 204488 null data\n",
      "------------------------------ 24 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 36 features have null data\n",
      "COMMONAREA_MODE have 214865 null data\n",
      "------------------------------ 25 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 35 features have null data\n",
      "ELEVATORS_MODE have 163891 null data\n",
      "------------------------------ 26 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 34 features have null data\n",
      "ENTRANCES_MODE have 154828 null data\n",
      "------------------------------ 27 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 33 features have null data\n",
      "FLOORSMAX_MODE have 153020 null data\n",
      "------------------------------ 28 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 32 features have null data\n",
      "FLOORSMIN_MODE have 208642 null data\n",
      "------------------------------ 29 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 31 features have null data\n",
      "LANDAREA_MODE have 182590 null data\n",
      "------------------------------ 30 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 30 features have null data\n",
      "LIVINGAPARTMENTS_MODE have 210199 null data\n",
      "------------------------------ 31 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 29 features have null data\n",
      "LIVINGAREA_MODE have 154350 null data\n",
      "------------------------------ 32 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 28 features have null data\n",
      "NONLIVINGAPARTMENTS_MODE have 213514 null data\n",
      "------------------------------ 33 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 27 features have null data\n",
      "NONLIVINGAREA_MODE have 169682 null data\n",
      "------------------------------ 34 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 26 features have null data\n",
      "APARTMENTS_MEDI have 156061 null data\n",
      "------------------------------ 35 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 25 features have null data\n",
      "BASEMENTAREA_MEDI have 179943 null data\n",
      "------------------------------ 36 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 24 features have null data\n",
      "YEARS_BEGINEXPLUATATION_MEDI have 150007 null data\n",
      "------------------------------ 37 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 23 features have null data\n",
      "YEARS_BUILD_MEDI have 204488 null data\n",
      "------------------------------ 38 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 22 features have null data\n",
      "COMMONAREA_MEDI have 214865 null data\n",
      "------------------------------ 39 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 21 features have null data\n",
      "ELEVATORS_MEDI have 163891 null data\n",
      "------------------------------ 40 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 20 features have null data\n",
      "ENTRANCES_MEDI have 154828 null data\n",
      "------------------------------ 41 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 19 features have null data\n",
      "FLOORSMAX_MEDI have 153020 null data\n",
      "------------------------------ 42 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 18 features have null data\n",
      "FLOORSMIN_MEDI have 208642 null data\n",
      "------------------------------ 43 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 17 features have null data\n",
      "LANDAREA_MEDI have 182590 null data\n",
      "------------------------------ 44 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 16 features have null data\n",
      "LIVINGAPARTMENTS_MEDI have 210199 null data\n",
      "------------------------------ 45 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 15 features have null data\n",
      "LIVINGAREA_MEDI have 154350 null data\n",
      "------------------------------ 46 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 14 features have null data\n",
      "NONLIVINGAPARTMENTS_MEDI have 213514 null data\n",
      "------------------------------ 47 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 13 features have null data\n",
      "NONLIVINGAREA_MEDI have 169682 null data\n",
      "------------------------------ 48 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 12 features have null data\n",
      "TOTALAREA_MODE have 148431 null data\n",
      "------------------------------ 49 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 11 features have null data\n",
      "OBS_30_CNT_SOCIAL_CIRCLE have 1021 null data\n",
      "------------------------------ 50 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 10 features have null data\n",
      "DEF_30_CNT_SOCIAL_CIRCLE have 1021 null data\n",
      "------------------------------ 51 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 9 features have null data\n",
      "OBS_60_CNT_SOCIAL_CIRCLE have 1021 null data\n",
      "------------------------------ 52 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 8 features have null data\n",
      "DEF_60_CNT_SOCIAL_CIRCLE have 1021 null data\n",
      "------------------------------ 53 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 7 features have null data\n",
      "DAYS_LAST_PHONE_CHANGE have 1 null data\n",
      "------------------------------ 54 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 6 features have null data\n",
      "AMT_REQ_CREDIT_BUREAU_HOUR have 41519 null data\n",
      "------------------------------ 55 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 5 features have null data\n",
      "AMT_REQ_CREDIT_BUREAU_DAY have 41519 null data\n",
      "------------------------------ 56 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 4 features have null data\n",
      "AMT_REQ_CREDIT_BUREAU_WEEK have 41519 null data\n",
      "------------------------------ 57 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 3 features have null data\n",
      "AMT_REQ_CREDIT_BUREAU_MON have 41519 null data\n",
      "------------------------------ 58 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 2 features have null data\n",
      "AMT_REQ_CREDIT_BUREAU_QRT have 41519 null data\n",
      "------------------------------ 59 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n",
      "For now, 1 features have null data\n",
      "AMT_REQ_CREDIT_BUREAU_YEAR have 41519 null data\n",
      "------------------------------ 60 : Start Linear regression ------------------------------\n",
      "Prediction and concat\n"
     ]
    }
   ],
   "source": [
    "# fill NaN values with linear regression\n",
    "\n",
    "with_null = data.loc[:, data.isnull().any()]\n",
    "without_null = data.loc[:, data.notnull().all()]\n",
    "features_with_null = with_null.columns\n",
    "\n",
    "for i, temp_feature in enumerate(features_with_null):\n",
    "    print('For now, {} features have null data'.format(data.isnull().any().sum()))\n",
    "    print('{} have {} null data'.format(temp_feature, data[temp_feature].isnull().sum()))\n",
    "    \n",
    "    temp_train = without_null.copy()\n",
    "    temp_train[temp_feature] = with_null[temp_feature]\n",
    "    \n",
    "    new_train = temp_train.loc[temp_train[temp_feature].notnull(), :]\n",
    "    new_test = temp_train.loc[temp_train[temp_feature].isnull(), :]\n",
    "    \n",
    "    temp_target = new_train[temp_feature].values\n",
    "    \n",
    "    new_train.drop([temp_feature], axis=1, inplace=True)\n",
    "    new_test.drop([temp_feature], axis=1, inplace=True)\n",
    "    \n",
    "    print('-'*30,  '{} : Start Linear regression'.format(i), '-'*30)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(new_train, temp_target)\n",
    "    \n",
    "    temp_pred = lr.predict(new_test)\n",
    "\n",
    "    new_train[temp_feature] = temp_target\n",
    "    new_test[temp_feature] = temp_pred\n",
    "    print('Prediction and concat')\n",
    "    foo = pd.concat([new_train, new_test]).sort_index()\n",
    "    \n",
    "    data[temp_feature] = foo[temp_feature]\n",
    "    \n",
    "    del foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns containing NaN values\n"
     ]
    }
   ],
   "source": [
    "# check and return for any columns with NaN\n",
    "print(\n",
    "    f\"There are {len(data.columns[data.isna().any()])} columns containing NaN values\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Check dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all float columns are supposed to be floats\n",
    "\n",
    "float_columns = data.columns[data.dtypes == \"float64\"]\n",
    "\n",
    "data[float_columns]\n",
    "\n",
    "data[[\"DAYS_REGISTRATION\", \"CNT_FAM_MEMBERS\"]] = data[[\"DAYS_REGISTRATION\", \"CNT_FAM_MEMBERS\"]].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SK_ID_CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456251</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456252</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456253</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456254</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456255</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307511 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [100002, 100003, 100004, 100006, 100007, 100008, 100009, 100010, 100011, 100012, 100014, 100015, 100016, 100017, 100018, 100019, 100020, 100021, 100022, 100023, 100024, 100025, 100026, 100027, 100029, 100030, 100031, 100032, 100033, 100034, 100035, 100036, 100037, 100039, 100040, 100041, 100043, 100044, 100045, 100046, 100047, 100048, 100049, 100050, 100051, 100052, 100053, 100054, 100055, 100056, 100058, 100059, 100060, 100061, 100062, 100063, 100064, 100068, 100069, 100070, 100071, 100072, 100073, 100075, 100076, 100077, 100078, 100079, 100080, 100081, 100082, 100083, 100084, 100085, 100086, 100087, 100088, 100089, 100093, 100094, 100095, 100096, 100097, 100098, 100099, 100100, 100101, 100102, 100103, 100104, 100105, 100108, 100110, 100111, 100112, 100113, 100114, 100115, 100116, 100118, ...]\n",
       "\n",
       "[307511 rows x 0 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets also check that all the values in the dataframe as numerical\n",
    "\n",
    "data.select_dtypes(exclude=[\"int64\", \"int32\", \"float64\", \"uint8\"])\n",
    "\n",
    "# this is just the index, so all is good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERSAMPLING\n",
    "x = data.drop([\"TARGET\"], axis=1)\n",
    "y = data.filter([\"TARGET\"], axis=1)\n",
    "\n",
    "smote = SMOTE()\n",
    "x_resampled, y_resampled = smote.fit_resample(x, y)\n",
    "\n",
    "oversampled_df = pd.DataFrame()\n",
    "oversampled_df = pd.concat([x_resampled, y_resampled], axis=1)\n",
    "# shuffle dataframe\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# oversampled_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_correlated_columns(data, threshold, column_dropped=False):\n",
    "    \"\"\"\n",
    "    This function drops any columns that have a high correlation to each other\n",
    "    It will only leave one of the highly correlated columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = data.corr().abs()\n",
    "    \n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    \n",
    "    # Find index of feature columns with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    # Drop the highly correlated columns\n",
    "    for column in to_drop:\n",
    "        data.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "    if column_dropped:\n",
    "        print(f\"Columns dropped:\\n{to_drop}\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565372, 245)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped:\n",
      "['AMT_GOODS_PRICE', 'FLAG_EMP_PHONE', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT_W_CITY', 'LIVE_REGION_NOT_WORK_REGION', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE', 'NAME_CONTRACT_TYPE_Revolving loans', 'NAME_INCOME_TYPE_Pensioner', 'ORGANIZATION_TYPE_XNA', 'EMERGENCYSTATE_MODE_No']\n"
     ]
    }
   ],
   "source": [
    "# drop any columns with a high correlation\n",
    "\n",
    "oversampled_df = drop_correlated_columns(oversampled_df, threshold=0.8, column_dropped=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565372, 203)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Split the dataset train/test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, test_size=0.2):\n",
    "    train_size = 1 - test_size\n",
    "    train = df[:int(len(df)*train_size)]\n",
    "    test = df[int(len(df)*train_size):]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: (480566, 203)\n",
      "Test dataset: (84806, 203)\n"
     ]
    }
   ],
   "source": [
    "oversampled_train, oversampled_test = split_dataset(oversampled_df, test_size=0.15)\n",
    "\n",
    "print(f\"Train dataset: {oversampled_train.shape}\")\n",
    "print(f\"Test dataset: {oversampled_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Export the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported\n"
     ]
    }
   ],
   "source": [
    "# export the cleaned data to a new file\n",
    "\n",
    "oversampled_train.to_csv(r\"processed_data/oversampled_train.csv\")\n",
    "oversampled_test.to_csv(r\"processed_data/oversampled_test.csv\")\n",
    "\n",
    "print(\"Data exported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "--- \n",
    "This notebook sets out to clean up the data\n",
    "\n",
    "Mainly, we are looking to encode any string columns, and fill any NaN entries, and balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"raw_data/application_train.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 121)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of the data\n",
    "data.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### NaN entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any columns with more than 80% missing\n",
    "\n",
    "data = data[data.columns[data.isnull().mean() < 0.80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For now, 67 features have null data\n",
      "AMT_ANNUITY have 12 null data\n",
      "------------------------------ 0 : Start Linear regression ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jack_\\AppData\\Local\\Temp\\ipykernel_16608\\1256960987.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_train.drop([temp_feature], axis=1, inplace=True)\n",
      "C:\\Users\\jack_\\AppData\\Local\\Temp\\ipykernel_16608\\1256960987.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_test.drop([temp_feature], axis=1, inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Cash loans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jack_\\OneDrive - University of Gloucestershire\\Financial Technology (2022-2023)\\final_project\\data\\data_preprocessing.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jack_/OneDrive%20-%20University%20of%20Gloucestershire/Financial%20Technology%20%282022-2023%29/final_project/data/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m,  \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m : Start Linear regression\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i), \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jack_/OneDrive%20-%20University%20of%20Gloucestershire/Financial%20Technology%20%282022-2023%29/final_project/data/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m lr \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jack_/OneDrive%20-%20University%20of%20Gloucestershire/Financial%20Technology%20%282022-2023%29/final_project/data/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m lr\u001b[39m.\u001b[39;49mfit(new_train, temp_target)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jack_/OneDrive%20-%20University%20of%20Gloucestershire/Financial%20Technology%20%282022-2023%29/final_project/data/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m temp_pred \u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39mpredict(new_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jack_/OneDrive%20-%20University%20of%20Gloucestershire/Financial%20Technology%20%282022-2023%29/final_project/data/data_preprocessing.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m new_train[temp_feature] \u001b[39m=\u001b[39m temp_target\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[0;32m    646\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 648\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    649\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    650\u001b[0m )\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Cash loans'"
     ]
    }
   ],
   "source": [
    "# fill NaN values with linear regression\n",
    "\n",
    "with_null = data.loc[:, data.isnull().any()]\n",
    "without_null = data.loc[:, data.notnull().all()]\n",
    "features_with_null = with_null.columns\n",
    "\n",
    "for i, temp_feature in enumerate(features_with_null):\n",
    "    print('For now, {} features have null data'.format(data.isnull().any().sum()))\n",
    "    print('{} have {} null data'.format(temp_feature, data[temp_feature].isnull().sum()))\n",
    "    \n",
    "    temp_train = without_null.copy()\n",
    "    temp_train[temp_feature] = with_null[temp_feature]\n",
    "    \n",
    "    new_train = temp_train.loc[temp_train[temp_feature].notnull(), :]\n",
    "    new_test = temp_train.loc[temp_train[temp_feature].isnull(), :]\n",
    "    \n",
    "    temp_target = new_train[temp_feature].values\n",
    "    \n",
    "    new_train.drop([temp_feature], axis=1, inplace=True)\n",
    "    new_test.drop([temp_feature], axis=1, inplace=True)\n",
    "    \n",
    "    print('-'*30,  '{} : Start Linear regression'.format(i), '-'*30)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(new_train, temp_target)\n",
    "    \n",
    "    temp_pred = lr.predict(new_test)\n",
    "\n",
    "    new_train[temp_feature] = temp_target\n",
    "    print('Prediction and concat')\n",
    "    \n",
    "    data[temp_feature] = new_train[temp_feature]\n",
    "    \n",
    "    del new_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 67 columns containing NaN values\n"
     ]
    }
   ],
   "source": [
    "# check and return for any columns with NaN\n",
    "print(\n",
    "    f\"There are {len(data.columns[data.isna().any()])} columns containing NaN values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fill NaN values with 0\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    df = df.fillna(value=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns containing NaN values\n"
     ]
    }
   ],
   "source": [
    "data = process_data(data)\n",
    "\n",
    "# check that there are not any NaN's (empty is good)\n",
    "nan_col = len(data.columns[data.isna().any()])\n",
    "\n",
    "print(f\"There are {nan_col} columns containing NaN values\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to created and implement an encoding dictionary\n",
    "\n",
    "\n",
    "def encode_data(data):\n",
    "    key = {}\n",
    "    key_columns = []\n",
    "\n",
    "    print(\"Creating encoding dictionary\")\n",
    "    # create a key for data in columns\n",
    "    for col in data.columns:\n",
    "        # check if the column is a string\n",
    "        if data[col].dtype == \"O\":\n",
    "            key_columns.append(col)\n",
    "            # loop over the unique strings in the dataframe\n",
    "            for col_name in data[col].unique():\n",
    "                # check if the string is not in the dictionary\n",
    "                if col_name not in key:\n",
    "                    # add to the dictionary with a unique value\n",
    "                    key[col_name] = len(key) + 1\n",
    "\n",
    "    print(\"Dictionary created...\")\n",
    "    print(\"Integrating keys into dataframe...\")\n",
    "    # replace the string values in the dataframe with the key created\n",
    "    for col in key_columns:\n",
    "        data = data.replace({str(col): key}) \n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    return data, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating encoding dictionary\n",
      "Dictionary created...\n",
      "Integrating keys into dataframe...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data, key = encode_data(data)\n",
    "# key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 columns that are not int or float\n"
     ]
    }
   ],
   "source": [
    "# check that all the columns are numbers (empty is good)\n",
    "col_dtype = len(\n",
    "    list(data.select_dtypes(exclude=[\"int64\", \"float64\"]).columns)\n",
    ")\n",
    "\n",
    "print(f\"There are {col_dtype} columns that are not int or float\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERSAMPLING\n",
    "x = data.drop([\"TARGET\"], axis=1)\n",
    "y = data.filter([\"TARGET\"], axis=1)\n",
    "\n",
    "smote = SMOTE()\n",
    "x_resampled, y_resampled = smote.fit_resample(x, y)\n",
    "\n",
    "oversampled_df = pd.DataFrame()\n",
    "oversampled_df = pd.concat([x_resampled, y_resampled], axis=1)\n",
    "# shuffle dataframe\n",
    "oversampled_df = oversampled_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# oversampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDERSAMPLING\n",
    "\n",
    "# split dataframe by defualt/non_default\n",
    "default = data[data[\"TARGET\"] == 1]\n",
    "non_default = data[data[\"TARGET\"] == 0]\n",
    "\n",
    "# reduce the larger sample to the smaller sample size\n",
    "non_default = non_default.sample(len(default), random_state=42)\n",
    "\n",
    "# add to new dataframe\n",
    "undersampled_df = pd.DataFrame()\n",
    "undersampled_df = pd.concat([default, non_default], axis=0)\n",
    "# shuffle dataframe\n",
    "undersampled_df = undersampled_df.sample(frac=1, random_state=42)\n",
    "\n",
    "# undersampled_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Split the dataset train/test csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, test_size=0.2):\n",
    "    train_size = 1 - test_size\n",
    "    train = df[:int(len(df)*train_size)]\n",
    "    test = df[int(len(df)*train_size):]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_train, undersampled_test = split_dataset(undersampled_df, test_size=0.15)\n",
    "oversampled_train, oversampled_test = split_dataset(oversampled_df, test_size=0.15)\n",
    "\n",
    "# oversampled_train.shape, oversampled_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Export the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported\n"
     ]
    }
   ],
   "source": [
    "# export the cleaned data to a new file\n",
    "\n",
    "# undersampled\n",
    "undersampled_train.to_csv(r\"processed_data/undersampled_train.csv\")\n",
    "undersampled_test.to_csv(r\"processed_data/undersampled_test.csv\")\n",
    "\n",
    "# oversampled\n",
    "oversampled_train.to_csv(r\"processed_data/oversampled_train.csv\")\n",
    "oversampled_test.to_csv(r\"processed_data/oversampled_test.csv\")\n",
    "\n",
    "print(\"Data exported\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
